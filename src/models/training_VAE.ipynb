{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92417428",
   "metadata": {},
   "source": [
    "# Training VAE\n",
    "\n",
    "In this code, we will be training the VAEs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c7dfb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "  \n",
    "# Open the file in binary mode\n",
    "with open('file.pkl', 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    myvar = pickle.load(file)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8f85bcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "20de3b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-28672, -28672, -28672, ...,    339,    348,    349],\n",
       "       [-28672, -28672, -28672, ...,    333,    334,    345],\n",
       "       [-28672, -28672, -28672, ...,    330,    331,    344],\n",
       "       ...,\n",
       "       [   301,    301,    281, ..., -28672, -28672, -28672],\n",
       "       [-28672,    279,    287, ..., -28672, -28672, -28672],\n",
       "       [   237,    231,    233, ..., -28672, -28672, -28672]], dtype=int16)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myvar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70088068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Sequence, Union, Any, Callable\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CelebA\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733056f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset_XONLY(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "#         self.y_list = y_list\n",
    "\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.array(self.file_list[idx])\n",
    "        img = np.where(img <0 , 0, img)/4089\n",
    "        img_transformed = self.transform(img)\n",
    "        \n",
    "        return img_transformed.float() , 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9a19aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                                              transforms.ToTensor(),\n",
    "                                                transforms.Resize(1024),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2a8ff212",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = SatelliteDataset_XONLY(myvar,train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "be047cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 1024])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bf67ff3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1272)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check[0][0].count_nonzero()/(1200* 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "33b417ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ksvd import ApproximateKSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a8c2ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.random.randn(1000, 20)\n",
    "# aksvd = ApproximateKSVD(n_components=128)\n",
    "# dictionary = aksvd.fit(listi[0]).components_\n",
    "# gamma = aksvd.transform(listi[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "aef92453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.07944131, -1.25793425, -0.49570015, ..., -1.75788319,\n",
       "        -0.11865563,  0.06634022],\n",
       "       [ 1.98229097,  0.00968416,  0.67403943, ...,  0.27335644,\n",
       "        -1.54414717,  1.92257779],\n",
       "       [-0.16015286, -1.16296824, -1.87841557, ..., -0.87469483,\n",
       "        -1.92479928, -0.80289057],\n",
       "       ...,\n",
       "       [-0.42783844,  0.30676393, -0.69683151, ...,  0.91008329,\n",
       "        -0.50888586,  0.67616166],\n",
       "       [ 1.25208091,  0.14527197, -0.59974859, ..., -0.6635284 ,\n",
       "        -0.41879842, -0.9293483 ],\n",
       "       [-0.87319302, -1.04516118,  1.13878109, ...,  0.99366363,\n",
       "        -1.59950658, -1.98102922]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7eecb630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 128)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1db561b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1200)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listi[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "abc1738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-28672, -28672, -28672, ...,    339,    348,    349],\n",
       "       [-28672, -28672, -28672, ...,    333,    334,    345],\n",
       "       [-28672, -28672, -28672, ...,    330,    331,    344],\n",
       "       ...,\n",
       "       [   301,    301,    281, ..., -28672, -28672, -28672],\n",
       "       [-28672,    279,    287, ..., -28672, -28672, -28672],\n",
       "       [   237,    231,    233, ..., -28672, -28672, -28672]], dtype=int16)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myvar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7ded76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "listi = [np.where(img <0 , 0, img)/4089 for img in myvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a25be111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [np.count_nonzero(i) for i in listi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4a08f9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d630e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a459d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = csr_matrix(listi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a7a26095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1200)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c271eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682afae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abdb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a71e65d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200*1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "512771ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .types_ import *\n",
    "from torch import nn\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input) :\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) :\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x, **kwargs) :\n",
    "        raise NotImplementedError\n",
    "\n",
    "#     @abstractmethod\n",
    "    def forward(self, *inputs) :\n",
    "        pass\n",
    "\n",
    "#     @abstractmethod\n",
    "    def loss_function(self, *inputs, **kwargs) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5a025459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "class BetaVAE(BaseVAE):\n",
    "\n",
    "    num_iter = 0 # Global static variable to keep track of iterations\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 beta: int = 4,\n",
    "                 gamma:float = 1000.,\n",
    "                 max_capacity: int = 25,\n",
    "                 Capacity_max_iter: int = 1e5,\n",
    "                 loss_type:str = 'B',\n",
    "                 **kwargs) -> None:\n",
    "        super(BetaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.loss_type = loss_type\n",
    "        self.C_max = torch.Tensor([max_capacity])\n",
    "        self.C_stop_iter = Capacity_max_iter\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [2,4,8,16,32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(2048, latent_dim)\n",
    "        self.fc_var = nn.Linear(2048, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 1,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z) :\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu, logvar) :\n",
    "        \"\"\"\n",
    "        Will a single z be enough ti compute the expectation\n",
    "        for the loss??\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        self.num_iter += 1\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        kld_weight = kwargs['M_N']  # Account for the minibatch samples from the dataset\n",
    "\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        if self.loss_type == 'H': # https://openreview.net/forum?id=Sy2fzU9gl\n",
    "            loss = recons_loss + self.beta * kld_weight * kld_loss\n",
    "        elif self.loss_type == 'B': # https://arxiv.org/pdf/1804.03599.pdf\n",
    "            self.C_max = self.C_max.to(input.device)\n",
    "            C = torch.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])\n",
    "            loss = recons_loss + self.gamma * kld_weight* (kld_loss - C).abs()\n",
    "        else:\n",
    "            raise ValueError('Undefined loss type.')\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':kld_loss}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) :\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x, **kwargs) :\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "# model = BetaVAE(**config['model_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "779c6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BetaVAE(**config['model_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a8648dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model_params':\n",
    "  {'name': 'BetaVAE',\n",
    "  'in_channels': 1,\n",
    "  'latent_dim': 128,\n",
    "  'loss_type': 'B',\n",
    "  'gamma': 10.0,\n",
    "  'max_capacity': 25,\n",
    "  'Capacity_max_iter': 10000},\n",
    "\n",
    "'data_params':\n",
    "  {'data_path': \"Data/\",\n",
    "  'train_batch_size': 64,\n",
    "  'val_batch_size':  64,\n",
    "  'patch_size': 64,\n",
    "  'num_workers': 4},\n",
    "  \n",
    "'exp_params':\n",
    "  {'LR': 0.005,\n",
    "  'weight_decay': 0.0,\n",
    "  'scheduler_gamma': 0.95,\n",
    "  'kld_weight': 0.00025,\n",
    "  'manual_seed': 1265},\n",
    "\n",
    "'trainer_params':\n",
    "  {\n",
    "      'gpus': [0],\n",
    "  'max_epochs': 10\n",
    "  },\n",
    "\n",
    "'logging_params':{\n",
    "    'save_dir': \"logs/\",\n",
    "  'manual_seed': 1265,\n",
    "  'name': 'BetaVAE'\n",
    "}\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "1c5a31fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024, 1024])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check[0][0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "a28f7bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1444"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38*38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "9705c2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024, 1024])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(check[0][0].unsqueeze(0))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "323684e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BetaVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (fc_var): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (decoder_input): Linear(in_features=128, out_features=2048, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): ConvTranspose2d(8, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): ConvTranspose2d(4, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): ConvTranspose2d(2, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc4287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "44912918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAEDataset(LightningDataModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning data module \n",
    "    Args:\n",
    "        data_dir: root directory of your dataset.\n",
    "        train_batch_size: the batch size to use during training.\n",
    "        val_batch_size: the batch size to use during validation.\n",
    "        patch_size: the size of the crop to take from the original images.\n",
    "        num_workers: the number of parallel workers to create to load data\n",
    "            items (see PyTorch's Dataloader documentation for more details).\n",
    "        pin_memory: whether prepared items should be loaded into pinned memory\n",
    "            or not. This can improve performance on GPUs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        train_batch_size: int = 8,\n",
    "        val_batch_size: int = 8,\n",
    "        patch_size: Union[int, Sequence[int]] = (256, 256),\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_path\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "#       =========================  OxfordPets Dataset  =========================\n",
    "            \n",
    "#         train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "#                                               transforms.CenterCrop(self.patch_size),\n",
    "# #                                               transforms.Resize(self.patch_size),\n",
    "#                                               transforms.ToTensor(),\n",
    "#                                                 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "        \n",
    "#         val_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "#                                             transforms.CenterCrop(self.patch_size),\n",
    "# #                                             transforms.Resize(self.patch_size),\n",
    "#                                             transforms.ToTensor(),\n",
    "#                                               transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "#         self.train_dataset = OxfordPets(\n",
    "#             self.data_dir,\n",
    "#             split='train',\n",
    "#             transform=train_transforms,\n",
    "#         )\n",
    "        \n",
    "#         self.val_dataset = OxfordPets(\n",
    "#             self.data_dir,\n",
    "#             split='val',\n",
    "#             transform=val_transforms,\n",
    "#         )\n",
    "        \n",
    "#       =========================  CelebA Dataset  =========================\n",
    "    \n",
    "        train_transforms = transforms.Compose([\n",
    "                                              transforms.ToTensor(),\n",
    "                                                transforms.Resize(1024),])\n",
    "        \n",
    "        val_transforms = transforms.Compose([\n",
    "                                              transforms.ToTensor(),\n",
    "                                                transforms.Resize(1024),])\n",
    "        \n",
    "        self.train_dataset = SatelliteDataset_XONLY(\n",
    "            X_train,\n",
    "            transform=train_transforms,\n",
    "        )\n",
    "        \n",
    "        # Replace CelebA with your dataset\n",
    "        self.val_dataset = SatelliteDataset_XONLY(\n",
    "            X_val,\n",
    "            transform=val_transforms\n",
    "        )\n",
    "#       ===============================================================\n",
    "        \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.val_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=144,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6198bdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: logs/BetaVAE\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "\n",
    "tb_logger =  TensorBoardLogger(save_dir=config['logging_params']['save_dir'],\n",
    "                               name=config['model_params']['name'],)\n",
    "runner = Trainer(logger=tb_logger,\n",
    "                 callbacks=[\n",
    "                     LearningRateMonitor(),\n",
    "                     ModelCheckpoint(save_top_k=2, \n",
    "                                     dirpath =os.path.join(tb_logger.log_dir , \"checkpoints\"), \n",
    "                                     monitor= \"val_loss\",\n",
    "                                     save_last= True),\n",
    "                 ],\n",
    "                 strategy=DDPPlugin(find_unused_parameters=False),\n",
    "                 **config['trainer_params'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "1e89d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import CelebA\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class VAEXperiment(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vae_model: BaseVAE,\n",
    "                 params: dict) -> None:\n",
    "        super(VAEXperiment, self).__init__()\n",
    "\n",
    "        self.model = vae_model\n",
    "        self.params = params\n",
    "        self.curr_device = None\n",
    "        self.hold_graph = False\n",
    "        try:\n",
    "            self.hold_graph = self.params['retain_first_backpass']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> Tensor:\n",
    "        return self.model(input, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx = 0):\n",
    "        real_img, labels = batch\n",
    "        self.curr_device = real_img.device\n",
    "\n",
    "        results = self.forward(real_img, labels = labels)\n",
    "        train_loss = self.model.loss_function(*results,\n",
    "                                              M_N = self.params['kld_weight'], #al_img.shape[0]/ self.num_train_imgs,\n",
    "                                              optimizer_idx=optimizer_idx,\n",
    "                                              batch_idx = batch_idx)\n",
    "\n",
    "        self.log_dict({key: val.item() for key, val in train_loss.items()}, sync_dist=True)\n",
    "\n",
    "        return train_loss['loss']\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, optimizer_idx = 0):\n",
    "        real_img, labels = batch\n",
    "        self.curr_device = real_img.device\n",
    "\n",
    "        results = self.forward(real_img, labels = labels)\n",
    "        val_loss = self.model.loss_function(*results,\n",
    "                                            M_N = 1.0, #real_img.shape[0]/ self.num_val_imgs,\n",
    "                                            optimizer_idx = optimizer_idx,\n",
    "                                            batch_idx = batch_idx)\n",
    "\n",
    "        self.log_dict({f\"val_{key}\": val.item() for key, val in val_loss.items()}, sync_dist=True)\n",
    "\n",
    "        \n",
    "    def on_validation_end(self) -> None:\n",
    "        self.sample_images()\n",
    "        \n",
    "    def sample_images(self):\n",
    "        # Get sample reconstruction image            \n",
    "        test_input, test_label = next(iter(self.trainer.datamodule.test_dataloader()))\n",
    "        test_input = test_input.to(self.curr_device)\n",
    "        test_label = test_label.to(self.curr_device)\n",
    "\n",
    "#         test_input, test_label = batch\n",
    "        recons = self.model.generate(test_input, labels = test_label)\n",
    "\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optims = []\n",
    "        scheds = []\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.params['LR'],\n",
    "                               weight_decay=self.params['weight_decay'])\n",
    "        optims.append(optimizer)\n",
    "        # Check if more than 1 optimizer is required (Used for adversarial training)\n",
    "        try:\n",
    "            if self.params['LR_2'] is not None:\n",
    "                optimizer2 = optim.Adam(getattr(self.model,self.params['submodel']).parameters(),\n",
    "                                        lr=self.params['LR_2'])\n",
    "                optims.append(optimizer2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if self.params['scheduler_gamma'] is not None:\n",
    "                scheduler = optim.lr_scheduler.ExponentialLR(optims[0],\n",
    "                                                             gamma = self.params['scheduler_gamma'])\n",
    "                scheds.append(scheduler)\n",
    "\n",
    "                # Check if another scheduler is required for the second optimizer\n",
    "                try:\n",
    "                    if self.params['scheduler_gamma_2'] is not None:\n",
    "                        scheduler2 = optim.lr_scheduler.ExponentialLR(optims[1],\n",
    "                                                                      gamma = self.params['scheduler_gamma_2'])\n",
    "                        scheds.append(scheduler2)\n",
    "                except:\n",
    "                    pass\n",
    "                return optims, scheds\n",
    "        except:\n",
    "            return optims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "50ef50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val = train_test_split( myvar, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "b1024bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = VAEXperiment(model,\n",
    "                          config['exp_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "1746da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = VAEDataset(**config[\"data_params\"], pin_memory=len(config['trainer_params']['gpus']) != 0)\n",
    "data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff871ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "3681e9b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | BetaVAE | 3.9 M \n",
      "----------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "15.756    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37645c67bc1c4657ab68790637bf3602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runner.fit(experiment, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3c6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6fd520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cadd31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c0e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
